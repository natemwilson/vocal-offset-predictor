{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "todo: replace zero padding with sentinel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import pytorch_pretrained_bert\n",
    "from pytorch_pretrained_bert import BertTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "from pytorch_pretrained_bert import BertForTokenClassification\n",
    "from pytorch_pretrained_bert import BertConfig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>word_id</th>\n",
       "      <th>word</th>\n",
       "      <th>start_time</th>\n",
       "      <th>end_time</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>tmp_offset</th>\n",
       "      <th>offset</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>token_id</th>\n",
       "      <th>sentence_length</th>\n",
       "      <th>first_token_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Mr</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.28</td>\n",
       "      <td>[mr]</td>\n",
       "      <td>[2720]</td>\n",
       "      <td>16</td>\n",
       "      <td>2720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>quilter</td>\n",
       "      <td>0.78</td>\n",
       "      <td>1.20</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.42</td>\n",
       "      <td>[quilt, ##er]</td>\n",
       "      <td>[27565, 2121]</td>\n",
       "      <td>16</td>\n",
       "      <td>27565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>is</td>\n",
       "      <td>1.20</td>\n",
       "      <td>1.39</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.19</td>\n",
       "      <td>[is]</td>\n",
       "      <td>[2003]</td>\n",
       "      <td>16</td>\n",
       "      <td>2003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>the</td>\n",
       "      <td>1.39</td>\n",
       "      <td>1.51</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.12</td>\n",
       "      <td>[the]</td>\n",
       "      <td>[1996]</td>\n",
       "      <td>16</td>\n",
       "      <td>1996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>apostle</td>\n",
       "      <td>1.51</td>\n",
       "      <td>2.08</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.57</td>\n",
       "      <td>[apostle]</td>\n",
       "      <td>[20121]</td>\n",
       "      <td>16</td>\n",
       "      <td>20121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>of</td>\n",
       "      <td>2.08</td>\n",
       "      <td>2.24</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.16</td>\n",
       "      <td>[of]</td>\n",
       "      <td>[1997]</td>\n",
       "      <td>16</td>\n",
       "      <td>1997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>the</td>\n",
       "      <td>2.24</td>\n",
       "      <td>2.34</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.10</td>\n",
       "      <td>[the]</td>\n",
       "      <td>[1996]</td>\n",
       "      <td>16</td>\n",
       "      <td>1996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>middle</td>\n",
       "      <td>2.34</td>\n",
       "      <td>2.58</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.24</td>\n",
       "      <td>[middle]</td>\n",
       "      <td>[2690]</td>\n",
       "      <td>16</td>\n",
       "      <td>2690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>classes</td>\n",
       "      <td>2.58</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.71</td>\n",
       "      <td>[classes]</td>\n",
       "      <td>[4280]</td>\n",
       "      <td>16</td>\n",
       "      <td>4280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>and</td>\n",
       "      <td>3.29</td>\n",
       "      <td>3.44</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.15</td>\n",
       "      <td>[and]</td>\n",
       "      <td>[1998]</td>\n",
       "      <td>16</td>\n",
       "      <td>1998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>we're</td>\n",
       "      <td>3.44</td>\n",
       "      <td>3.64</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.20</td>\n",
       "      <td>[we, ', re]</td>\n",
       "      <td>[2057, 1005, 2128]</td>\n",
       "      <td>16</td>\n",
       "      <td>2057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>glad</td>\n",
       "      <td>3.64</td>\n",
       "      <td>4.05</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.41</td>\n",
       "      <td>[glad]</td>\n",
       "      <td>[5580]</td>\n",
       "      <td>16</td>\n",
       "      <td>5580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>to</td>\n",
       "      <td>4.05</td>\n",
       "      <td>4.19</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.14</td>\n",
       "      <td>[to]</td>\n",
       "      <td>[2000]</td>\n",
       "      <td>16</td>\n",
       "      <td>2000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>welcome</td>\n",
       "      <td>4.19</td>\n",
       "      <td>4.54</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.35</td>\n",
       "      <td>[welcome]</td>\n",
       "      <td>[6160]</td>\n",
       "      <td>16</td>\n",
       "      <td>6160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>his</td>\n",
       "      <td>4.54</td>\n",
       "      <td>4.81</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.27</td>\n",
       "      <td>[his]</td>\n",
       "      <td>[2010]</td>\n",
       "      <td>16</td>\n",
       "      <td>2010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>gospel</td>\n",
       "      <td>4.81</td>\n",
       "      <td>5.46</td>\n",
       "      <td>0.87</td>\n",
       "      <td>-4.33</td>\n",
       "      <td>0.65</td>\n",
       "      <td>[gospel]</td>\n",
       "      <td>[8036]</td>\n",
       "      <td>16</td>\n",
       "      <td>8036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>nor</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.30</td>\n",
       "      <td>[nor]</td>\n",
       "      <td>[4496]</td>\n",
       "      <td>10</td>\n",
       "      <td>4496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>is</td>\n",
       "      <td>0.78</td>\n",
       "      <td>1.05</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.27</td>\n",
       "      <td>[is]</td>\n",
       "      <td>[2003]</td>\n",
       "      <td>10</td>\n",
       "      <td>2003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>Mister</td>\n",
       "      <td>1.05</td>\n",
       "      <td>1.35</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.30</td>\n",
       "      <td>[mister]</td>\n",
       "      <td>[12525]</td>\n",
       "      <td>10</td>\n",
       "      <td>12525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>Coulter's</td>\n",
       "      <td>1.35</td>\n",
       "      <td>1.80</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.45</td>\n",
       "      <td>[coulter, ', s]</td>\n",
       "      <td>[28293, 1005, 1055]</td>\n",
       "      <td>10</td>\n",
       "      <td>28293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>manner</td>\n",
       "      <td>1.80</td>\n",
       "      <td>2.23</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.43</td>\n",
       "      <td>[manner]</td>\n",
       "      <td>[5450]</td>\n",
       "      <td>10</td>\n",
       "      <td>5450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>less</td>\n",
       "      <td>2.23</td>\n",
       "      <td>2.55</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.35</td>\n",
       "      <td>[less]</td>\n",
       "      <td>[2625]</td>\n",
       "      <td>10</td>\n",
       "      <td>2625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>interesting</td>\n",
       "      <td>2.58</td>\n",
       "      <td>3.22</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.64</td>\n",
       "      <td>[interesting]</td>\n",
       "      <td>[5875]</td>\n",
       "      <td>10</td>\n",
       "      <td>5875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>than</td>\n",
       "      <td>3.22</td>\n",
       "      <td>3.50</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.28</td>\n",
       "      <td>[than]</td>\n",
       "      <td>[2084]</td>\n",
       "      <td>10</td>\n",
       "      <td>2084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>his</td>\n",
       "      <td>3.50</td>\n",
       "      <td>3.81</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.34</td>\n",
       "      <td>[his]</td>\n",
       "      <td>[2010]</td>\n",
       "      <td>10</td>\n",
       "      <td>2010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>matter</td>\n",
       "      <td>3.84</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.89</td>\n",
       "      <td>-3.46</td>\n",
       "      <td>0.54</td>\n",
       "      <td>[matter]</td>\n",
       "      <td>[3043]</td>\n",
       "      <td>10</td>\n",
       "      <td>3043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>he</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.28</td>\n",
       "      <td>[he]</td>\n",
       "      <td>[2002]</td>\n",
       "      <td>30</td>\n",
       "      <td>2002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>tells</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.33</td>\n",
       "      <td>[tells]</td>\n",
       "      <td>[4136]</td>\n",
       "      <td>30</td>\n",
       "      <td>4136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>us</td>\n",
       "      <td>0.99</td>\n",
       "      <td>1.25</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.29</td>\n",
       "      <td>[us]</td>\n",
       "      <td>[2149]</td>\n",
       "      <td>30</td>\n",
       "      <td>2149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>that</td>\n",
       "      <td>1.28</td>\n",
       "      <td>1.56</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.31</td>\n",
       "      <td>[that]</td>\n",
       "      <td>[2008]</td>\n",
       "      <td>30</td>\n",
       "      <td>2008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>at</td>\n",
       "      <td>1.59</td>\n",
       "      <td>1.77</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.18</td>\n",
       "      <td>[at]</td>\n",
       "      <td>[2012]</td>\n",
       "      <td>30</td>\n",
       "      <td>2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>this</td>\n",
       "      <td>1.77</td>\n",
       "      <td>2.02</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>[this]</td>\n",
       "      <td>[2023]</td>\n",
       "      <td>30</td>\n",
       "      <td>2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>festive</td>\n",
       "      <td>2.02</td>\n",
       "      <td>2.52</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>[fest, ##ive]</td>\n",
       "      <td>[17037, 3512]</td>\n",
       "      <td>30</td>\n",
       "      <td>17037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>season</td>\n",
       "      <td>2.52</td>\n",
       "      <td>3.02</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>[season]</td>\n",
       "      <td>[2161]</td>\n",
       "      <td>30</td>\n",
       "      <td>2161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>of</td>\n",
       "      <td>3.02</td>\n",
       "      <td>3.18</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.16</td>\n",
       "      <td>[of]</td>\n",
       "      <td>[1997]</td>\n",
       "      <td>30</td>\n",
       "      <td>1997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>the</td>\n",
       "      <td>3.18</td>\n",
       "      <td>3.32</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.14</td>\n",
       "      <td>[the]</td>\n",
       "      <td>[1996]</td>\n",
       "      <td>30</td>\n",
       "      <td>1996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>year</td>\n",
       "      <td>3.32</td>\n",
       "      <td>3.77</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.79</td>\n",
       "      <td>[year]</td>\n",
       "      <td>[2095]</td>\n",
       "      <td>30</td>\n",
       "      <td>2095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>with</td>\n",
       "      <td>4.11</td>\n",
       "      <td>4.40</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.29</td>\n",
       "      <td>[with]</td>\n",
       "      <td>[2007]</td>\n",
       "      <td>30</td>\n",
       "      <td>2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>Christmas</td>\n",
       "      <td>4.40</td>\n",
       "      <td>5.01</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.61</td>\n",
       "      <td>[christmas]</td>\n",
       "      <td>[4234]</td>\n",
       "      <td>30</td>\n",
       "      <td>4234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>and</td>\n",
       "      <td>5.01</td>\n",
       "      <td>5.18</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.17</td>\n",
       "      <td>[and]</td>\n",
       "      <td>[1998]</td>\n",
       "      <td>30</td>\n",
       "      <td>1998</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    sentence_id  word_id         word  start_time  end_time  accuracy  \\\n",
       "0             1        1           Mr        0.50      0.78      0.87   \n",
       "1             1        2      quilter        0.78      1.20      0.87   \n",
       "2             1        3           is        1.20      1.39      0.87   \n",
       "3             1        4          the        1.39      1.51      0.87   \n",
       "4             1        5      apostle        1.51      2.08      0.87   \n",
       "5             1        6           of        2.08      2.24      0.87   \n",
       "6             1        7          the        2.24      2.34      0.87   \n",
       "7             1        8       middle        2.34      2.58      0.87   \n",
       "8             1        9      classes        2.58      3.26      0.87   \n",
       "9             1       10          and        3.29      3.44      0.87   \n",
       "10            1       11        we're        3.44      3.64      0.87   \n",
       "11            1       12         glad        3.64      4.05      0.87   \n",
       "12            1       13           to        4.05      4.19      0.87   \n",
       "13            1       14      welcome        4.19      4.54      0.87   \n",
       "14            1       15          his        4.54      4.81      0.87   \n",
       "15            1       16       gospel        4.81      5.46      0.87   \n",
       "16            2        1          nor        0.48      0.78      0.89   \n",
       "17            2        2           is        0.78      1.05      0.89   \n",
       "18            2        3       Mister        1.05      1.35      0.89   \n",
       "19            2        4    Coulter's        1.35      1.80      0.89   \n",
       "20            2        5       manner        1.80      2.23      0.89   \n",
       "21            2        6         less        2.23      2.55      0.89   \n",
       "22            2        7  interesting        2.58      3.22      0.89   \n",
       "23            2        8         than        3.22      3.50      0.89   \n",
       "24            2        9          his        3.50      3.81      0.89   \n",
       "25            2       10       matter        3.84      4.38      0.89   \n",
       "26            3        1           he        0.38      0.66      0.90   \n",
       "27            3        2        tells        0.66      0.99      0.90   \n",
       "28            3        3           us        0.99      1.25      0.90   \n",
       "29            3        4         that        1.28      1.56      0.90   \n",
       "30            3        5           at        1.59      1.77      0.90   \n",
       "31            3        6         this        1.77      2.02      0.90   \n",
       "32            3        7      festive        2.02      2.52      0.90   \n",
       "33            3        8       season        2.52      3.02      0.90   \n",
       "34            3        9           of        3.02      3.18      0.90   \n",
       "35            3       10          the        3.18      3.32      0.90   \n",
       "36            3       11         year        3.32      3.77      0.90   \n",
       "37            3       12         with        4.11      4.40      0.90   \n",
       "38            3       13    Christmas        4.40      5.01      0.90   \n",
       "39            3       14          and        5.01      5.18      0.90   \n",
       "\n",
       "    tmp_offset  offset        tokenized             token_id  sentence_length  \\\n",
       "0         0.28    0.28             [mr]               [2720]               16   \n",
       "1         0.42    0.42    [quilt, ##er]        [27565, 2121]               16   \n",
       "2         0.19    0.19             [is]               [2003]               16   \n",
       "3         0.12    0.12            [the]               [1996]               16   \n",
       "4         0.57    0.57        [apostle]              [20121]               16   \n",
       "5         0.16    0.16             [of]               [1997]               16   \n",
       "6         0.10    0.10            [the]               [1996]               16   \n",
       "7         0.24    0.24         [middle]               [2690]               16   \n",
       "8         0.71    0.71        [classes]               [4280]               16   \n",
       "9         0.15    0.15            [and]               [1998]               16   \n",
       "10        0.20    0.20      [we, ', re]   [2057, 1005, 2128]               16   \n",
       "11        0.41    0.41           [glad]               [5580]               16   \n",
       "12        0.14    0.14             [to]               [2000]               16   \n",
       "13        0.35    0.35        [welcome]               [6160]               16   \n",
       "14        0.27    0.27            [his]               [2010]               16   \n",
       "15       -4.33    0.65         [gospel]               [8036]               16   \n",
       "16        0.30    0.30            [nor]               [4496]               10   \n",
       "17        0.27    0.27             [is]               [2003]               10   \n",
       "18        0.30    0.30         [mister]              [12525]               10   \n",
       "19        0.45    0.45  [coulter, ', s]  [28293, 1005, 1055]               10   \n",
       "20        0.43    0.43         [manner]               [5450]               10   \n",
       "21        0.35    0.35           [less]               [2625]               10   \n",
       "22        0.64    0.64    [interesting]               [5875]               10   \n",
       "23        0.28    0.28           [than]               [2084]               10   \n",
       "24        0.34    0.34            [his]               [2010]               10   \n",
       "25       -3.46    0.54         [matter]               [3043]               10   \n",
       "26        0.28    0.28             [he]               [2002]               30   \n",
       "27        0.33    0.33          [tells]               [4136]               30   \n",
       "28        0.29    0.29             [us]               [2149]               30   \n",
       "29        0.31    0.31           [that]               [2008]               30   \n",
       "30        0.18    0.18             [at]               [2012]               30   \n",
       "31        0.25    0.25           [this]               [2023]               30   \n",
       "32        0.50    0.50    [fest, ##ive]        [17037, 3512]               30   \n",
       "33        0.50    0.50         [season]               [2161]               30   \n",
       "34        0.16    0.16             [of]               [1997]               30   \n",
       "35        0.14    0.14            [the]               [1996]               30   \n",
       "36        0.79    0.79           [year]               [2095]               30   \n",
       "37        0.29    0.29           [with]               [2007]               30   \n",
       "38        0.61    0.61      [christmas]               [4234]               30   \n",
       "39        0.17    0.17            [and]               [1998]               30   \n",
       "\n",
       "    first_token_id  \n",
       "0             2720  \n",
       "1            27565  \n",
       "2             2003  \n",
       "3             1996  \n",
       "4            20121  \n",
       "5             1997  \n",
       "6             1996  \n",
       "7             2690  \n",
       "8             4280  \n",
       "9             1998  \n",
       "10            2057  \n",
       "11            5580  \n",
       "12            2000  \n",
       "13            6160  \n",
       "14            2010  \n",
       "15            8036  \n",
       "16            4496  \n",
       "17            2003  \n",
       "18           12525  \n",
       "19           28293  \n",
       "20            5450  \n",
       "21            2625  \n",
       "22            5875  \n",
       "23            2084  \n",
       "24            2010  \n",
       "25            3043  \n",
       "26            2002  \n",
       "27            4136  \n",
       "28            2149  \n",
       "29            2008  \n",
       "30            2012  \n",
       "31            2023  \n",
       "32           17037  \n",
       "33            2161  \n",
       "34            1997  \n",
       "35            1996  \n",
       "36            2095  \n",
       "37            2007  \n",
       "38            4234  \n",
       "39            1998  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('output_30.csv')\n",
    "data.columns = ['sentence_id', 'word_id', 'word', 'start_time', 'end_time', 'accuracy']\n",
    "data['tmp_offset'] = -1 * (data.start_time - data.start_time.shift(-1))\n",
    "data['offset'] = data.apply(lambda row: row['tmp_offset'] if row['tmp_offset'] >= 0 else row['end_time'] - row['start_time'], axis=1)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "data['tokenized'] = data.word.apply(tokenizer.tokenize)\n",
    "data['token_id'] = data.tokenized.apply(tokenizer.convert_tokens_to_ids)\n",
    "data['sentence_length'] = data.sentence_id.map(data.groupby('sentence_id').count().token_id)\n",
    "data.loc[:, 'first_token_id'] = data.token_id.map(lambda x: x[0])\n",
    "\n",
    "data.head(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordsDataset:\n",
    "    def __init__(self, dataframe):\n",
    "        self.data = dataframe\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        return (np.expand_dims(self.data.iloc[item].token_id[0], axis=1), \n",
    "                np.expand_dims(self.data.iloc[item].offset,      axis=1))\n",
    "    \n",
    "class SentencesDataset:\n",
    "    def __init__(self, dataframe, maximum_sentence_length=None):\n",
    "        self.data = dataframe\n",
    "        if maximum_sentence_length:\n",
    "            self.maximum_sentence_length = maximum_sentence_length\n",
    "        else:\n",
    "            self.maximum_sentence_length = self.data.sentence_length.max()\n",
    "        print(f\"max sentence length set to {self.maximum_sentence_length}.\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.data.sentence_id.max()\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        unpadded_ids =     self.data[data.sentence_id == item].first_token_id.values\n",
    "        unpadded_offsets = self.data[data.sentence_id == item].offset.values\n",
    "        \n",
    "        if len(unpadded_ids) == self.maximum_sentence_length:\n",
    "            return unpadded_ids.astype('long'), unpadded_offsets\n",
    "\n",
    "        leftover_space = self.maximum_sentence_length - len(unpadded_ids)\n",
    "        padding = np.zeros(leftover_space)\n",
    "\n",
    "        \n",
    "        padded_ids = np.concatenate([unpadded_ids, padding]).astype('long')\n",
    "        padded_offsets = np.concatenate([unpadded_offsets, padding])\n",
    "        \n",
    "        return padded_ids, padded_offsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns; sns.set(rc={'figure.figsize':(11.7,8.27)})\n",
    "data.sentence_length.hist(bins=40)\n",
    "data.sentence_length.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4124 20.1188166828322 1 269\n",
      "4116 21.044703595724005 270 539\n",
      "46461 21.33602376186479 540 3392\n"
     ]
    }
   ],
   "source": [
    "data_val = data[data.sentence_id < 270]\n",
    "data_tst = data[(data.sentence_id >= 270) & (data.sentence_id < 540)]\n",
    "data_trn = data[data.sentence_id >= 540]\n",
    "\n",
    "\n",
    "print(len(data_val), data_val.sentence_length.mean(), data_val.sentence_id.min(), data_val.sentence_id.max())\n",
    "print(len(data_tst), data_tst.sentence_length.mean(), data_tst.sentence_id.min(), data_tst.sentence_id.max())\n",
    "print(len(data_trn), data_trn.sentence_length.mean(), data_trn.sentence_id.min(), data_trn.sentence_id.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForTokenClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(32000, 768)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1)\n",
       "  (classifier): Linear(in_features=768, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = BertConfig(vocab_size_or_config_json_file=32000, \n",
    "                    hidden_size=768,\n",
    "                    num_hidden_layers=12, \n",
    "                    num_attention_heads=12, \n",
    "                    intermediate_size=3072,)\n",
    "\n",
    "model = BertForTokenClassification(config, 1)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max sentence length set to 30.\n",
      "max sentence length set to 30.\n",
      "max sentence length set to 30.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/ipykernel_launcher.py:25: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "/usr/lib/python3.6/site-packages/ipykernel_launcher.py:26: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/105"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m----------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-8267c7b1f928>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0msum_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader_trn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\r{batch_index}/105\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib64/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    613\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 615\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    616\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib64/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    613\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 615\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    616\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-32be9a7e8921>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0munpadded_ids\u001b[0m \u001b[0;34m=\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentence_id\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfirst_token_id\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0munpadded_offsets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentence_id\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moffset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib64/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2916\u001b[0m         \u001b[0;31m# Do we have a (boolean) 1d indexer?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2917\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_bool_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2918\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_bool_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2920\u001b[0m         \u001b[0;31m# We are left with two options: a single key, and a collection of keys,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib64/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_getitem_bool_array\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2967\u001b[0m         \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_bool_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2968\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2969\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_take\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2970\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2971\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib64/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_take\u001b[0;34m(self, indices, axis, is_copy)\u001b[0m\n\u001b[1;32m   3357\u001b[0m         new_data = self._data.take(indices,\n\u001b[1;32m   3358\u001b[0m                                    \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_block_manager_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3359\u001b[0;31m                                    verify=True)\n\u001b[0m\u001b[1;32m   3360\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_constructor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__finalize__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib64/python3.6/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, indexer, axis, verify, convert)\u001b[0m\n\u001b[1;32m   1348\u001b[0m         \u001b[0mnew_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m         return self.reindex_indexer(new_axis=new_labels, indexer=indexer,\n\u001b[0;32m-> 1350\u001b[0;31m                                     axis=axis, allow_dups=True)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlsuffix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrsuffix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib64/python3.6/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mreindex_indexer\u001b[0;34m(self, new_axis, indexer, axis, fill_value, allow_dups, copy)\u001b[0m\n\u001b[1;32m   1233\u001b[0m             new_blocks = [blk.take_nd(indexer, axis=axis, fill_tuple=(\n\u001b[1;32m   1234\u001b[0m                 fill_value if fill_value is not None else blk.fill_value,))\n\u001b[0;32m-> 1235\u001b[0;31m                 for blk in self.blocks]\n\u001b[0m\u001b[1;32m   1236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1237\u001b[0m         \u001b[0mnew_axes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib64/python3.6/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1233\u001b[0m             new_blocks = [blk.take_nd(indexer, axis=axis, fill_tuple=(\n\u001b[1;32m   1234\u001b[0m                 fill_value if fill_value is not None else blk.fill_value,))\n\u001b[0;32m-> 1235\u001b[0;31m                 for blk in self.blocks]\n\u001b[0m\u001b[1;32m   1236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1237\u001b[0m         \u001b[0mnew_axes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib64/python3.6/site-packages/pandas/core/internals/blocks.py\u001b[0m in \u001b[0;36mtake_nd\u001b[0;34m(self, indexer, axis, new_mgr_locs, fill_tuple)\u001b[0m\n\u001b[1;32m   1236\u001b[0m             \u001b[0mfill_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfill_tuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1237\u001b[0m             new_values = algos.take_nd(values, indexer, axis=axis,\n\u001b[0;32m-> 1238\u001b[0;31m                                        allow_fill=True, fill_value=fill_value)\n\u001b[0m\u001b[1;32m   1239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1240\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnew_mgr_locs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib64/python3.6/site-packages/pandas/core/algorithms.py\u001b[0m in \u001b[0;36mtake_nd\u001b[0;34m(arr, indexer, axis, out, fill_value, mask_info, allow_fill)\u001b[0m\n\u001b[1;32m   1580\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mis_datetime64tz_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1581\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfill_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_fill\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mallow_fill\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1582\u001b[0;31m     \u001b[0;32melif\u001b[0m \u001b[0mis_interval_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1583\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfill_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_fill\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mallow_fill\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib64/python3.6/site-packages/pandas/core/dtypes/common.py\u001b[0m in \u001b[0;36mis_interval_dtype\u001b[0;34m(arr_or_dtype)\u001b[0m\n\u001b[1;32m    567\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0marr_or_dtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    568\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 569\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mIntervalDtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr_or_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    570\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib64/python3.6/site-packages/pandas/core/dtypes/dtypes.py\u001b[0m in \u001b[0;36mis_dtype\u001b[0;34m(cls, dtype)\u001b[0m\n\u001b[1;32m    984\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    985\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 986\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mIntervalDtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/lib64/python3.6/site-packages/pandas/core/dtypes/base.py\u001b[0m in \u001b[0;36mis_dtype\u001b[0;34m(cls, dtype)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         if isinstance(dtype, (ABCSeries, ABCIndexClass,\n\u001b[0;32m--> 101\u001b[0;31m                               ABCDataFrame, np.dtype)):\n\u001b[0m\u001b[1;32m    102\u001b[0m             \u001b[0;31m# https://github.com/pandas-dev/pandas/issues/22960\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m             \u001b[0;31m# avoid passing data to `construct_from_string`. This could\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import datetime\n",
    "\n",
    "\n",
    "full_dataset_max = data.sentence_length.max()\n",
    "\n",
    "\n",
    "dataset_val = SentencesDataset(data_val, maximum_sentence_length=full_dataset_max)\n",
    "dataset_trn = SentencesDataset(data_trn, maximum_sentence_length=full_dataset_max)\n",
    "dataset_tst = SentencesDataset(data_tst, maximum_sentence_length=full_dataset_max)\n",
    "\n",
    "\n",
    "\n",
    "loader_val = DataLoader(dataset_val, batch_size=32)\n",
    "loader_trn = DataLoader(dataset_trn, batch_size=32)\n",
    "loader_tst = DataLoader(dataset_tst, batch_size=32)\n",
    "\n",
    "\n",
    "\n",
    "config = BertConfig(vocab_size_or_config_json_file=32000, \n",
    "                    hidden_size=768,\n",
    "                    num_hidden_layers=12, \n",
    "                    num_attention_heads=12, \n",
    "                    intermediate_size=3072,)\n",
    "\n",
    "model = BertForTokenClassification(config, 1)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "criterion = torch.nn.SmoothL1Loss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.0003, momentum=0.9)\n",
    "\n",
    "# 70 / 15 / 15\n",
    "\n",
    "epochs = 1 # recommended 3200???\n",
    "#TODO: add pickling logic\n",
    "for epoch in range(epochs):\n",
    "    sum_loss = 0\n",
    "    start_time = datetime.datetime.now()\n",
    "    for batch_index, (inputs, targets) in enumerate(loader_trn):\n",
    "        print(f\"\\r{batch_index}/105\", end=\"\")\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(inputs).squeeze(-1)\n",
    "        loss = criterion(logits, targets.float())\n",
    "        sum_loss += loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    end_time = datetime.datetime.now()\n",
    "    print(f\"\\repoch: {epoch} loss: {sum_loss:4f} elapsed time: {end_time - start_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_performance(model, loader, criterion):\n",
    "    with torch.no_grad():\n",
    "        sum_loss = 0\n",
    "        for batch_index, (inputs, targets) in enumerate(loader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            logits = model(inputs).squeeze(-1)\n",
    "            loss = criterion(logits, targets.float())\n",
    "            sum_loss += loss\n",
    "    print(f\"sum loss: {sum_loss}\")\n",
    "    print(\"printing distribution of predictions and labels of batch 32\")\n",
    "    preds = []\n",
    "    for i in logits:\n",
    "        for j in i:\n",
    "            preds.append(j.item())\n",
    "    labels = []\n",
    "    for i in targets:\n",
    "        for j in i:\n",
    "            labels.append(j.item())\n",
    "    print(preds)\n",
    "    print(labels)\n",
    "    return preds, labels\n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = []\n",
    "for batch_index, (inputs, targets) in enumerate(loader):\n",
    "    with torch.no_grad():\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        outputs.append(model(inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    sum_loss = 0\n",
    "    for batch_index, (inputs, targets) in enumerate(loader_val):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        logits = model(inputs).squeeze(-1)\n",
    "        loss = criterion(logits, targets.float())\n",
    "        sum_loss += loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'bert_model_50_epochs_lr_0.0003.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "sns.distplot(preds)\n",
    "plt.title('Batch 1 Predictions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = []\n",
    "for i in logits:\n",
    "    for j in i:\n",
    "        preds.append(j.item())\n",
    "labels = []\n",
    "for i in targets:\n",
    "    for j in i:\n",
    "        labels.append(j.item())\n",
    "\n",
    "f, axes = plt.subplots(2, 1, figsize=(20, 7), sharex=True)\n",
    "ax_top = sns.distplot(labels, ax=axes[0])\n",
    "ax_bot = sns.distplot(preds, ax=axes[1])\n",
    "ax_top.set_title('Labels')\n",
    "ax_bot.set_title('Predictions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/ipykernel_launcher.py:25: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "/usr/lib/python3.6/site-packages/ipykernel_launcher.py:26: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sum loss: 0.4797042906284332\n",
      "printing distribution of predictions and labels of batch 32\n",
      "[0.15689770877361298, 0.15689772367477417, 0.1568976193666458, 0.15689750015735626, 0.15689793229103088, 0.15689760446548462, 0.1568976640701294, 0.15689772367477417, 0.15689782798290253, 0.15689797699451447, 0.15689754486083984, 0.15689781308174133, 0.15689796209335327, 0.1568976491689682, 0.15689747035503387, 0.15689757466316223, 0.1568974405527115, 0.1568976193666458, 0.1568978875875473, 0.15689750015735626, 0.1568976789712906, 0.1568973958492279, 0.1568978726863861, 0.1568979024887085, 0.15689781308174133, 0.15689778327941895, 0.15689784288406372, 0.15689782798290253, 0.1568978875875473, 0.15689782798290253, 0.15689735114574432, 0.15689726173877716, 0.15689706802368164, 0.15689723193645477, 0.15689754486083984, 0.15689730644226074, 0.15689711272716522, 0.15689727663993835, 0.15689747035503387, 0.15689754486083984, 0.1568974405527115, 0.15689745545387268, 0.15689754486083984, 0.15689751505851746, 0.15689732134342194, 0.1568974107503891, 0.15689736604690552, 0.1568974405527115, 0.15689758956432343, 0.1568973809480667, 0.15689752995967865, 0.15689736604690552, 0.15689754486083984, 0.15689755976200104, 0.15689745545387268, 0.1568974405527115, 0.15689750015735626, 0.15689747035503387, 0.15689752995967865, 0.15689748525619507, 0.15689709782600403, 0.15689723193645477, 0.15689735114574432, 0.15689709782600403, 0.1568974256515503, 0.15689709782600403, 0.15689720213413239, 0.15689717233181, 0.15689729154109955, 0.15689735114574432, 0.15689729154109955, 0.15689733624458313, 0.1568974256515503, 0.1568973809480667, 0.1568971872329712, 0.15689727663993835, 0.15689724683761597, 0.15689730644226074, 0.15689745545387268, 0.15689723193645477, 0.1568973958492279, 0.15689723193645477, 0.1568974107503891, 0.1568974405527115, 0.15689733624458313, 0.15689732134342194, 0.15689736604690552, 0.15689735114574432, 0.1568973958492279, 0.15689735114574432, 0.15689782798290253, 0.15689772367477417, 0.15689781308174133, 0.15689781308174133, 0.15689799189567566, 0.15689754486083984, 0.15689779818058014, 0.15689769387245178, 0.15689778327941895, 0.15689802169799805, 0.15689754486083984, 0.15689773857593536, 0.1568979024887085, 0.15689769387245178, 0.1568976193666458, 0.15689769387245178, 0.15689758956432343, 0.15689769387245178, 0.15689800679683685, 0.15689754486083984, 0.1568978875875473, 0.15689772367477417, 0.15689793229103088, 0.15689794719219208, 0.15689785778522491, 0.15689782798290253, 0.1568978875875473, 0.15689785778522491, 0.1568979173898697, 0.15689785778522491, 0.15689782798290253, 0.15689772367477417, 0.15689781308174133, 0.15689781308174133, 0.15689799189567566, 0.15689754486083984, 0.15689779818058014, 0.15689769387245178, 0.15689778327941895, 0.15689802169799805, 0.15689754486083984, 0.15689773857593536, 0.1568979024887085, 0.15689769387245178, 0.1568976193666458, 0.15689769387245178, 0.15689758956432343, 0.15689769387245178, 0.15689800679683685, 0.15689754486083984, 0.1568978875875473, 0.15689772367477417, 0.15689793229103088, 0.15689794719219208, 0.15689785778522491, 0.15689782798290253, 0.1568978875875473, 0.15689785778522491, 0.1568979173898697, 0.15689785778522491, 0.15689758956432343, 0.1568976491689682, 0.1568976789712906, 0.1568974256515503, 0.1568978726863861, 0.15689760446548462, 0.1568973809480667, 0.15689751505851746, 0.15689757466316223, 0.1568976491689682, 0.1568974107503891, 0.1568976789712906, 0.15689772367477417, 0.15689760446548462, 0.1568974256515503, 0.156897634267807, 0.1568976193666458, 0.1568976789712906, 0.15689782798290253, 0.15689760446548462, 0.15689776837825775, 0.15689760446548462, 0.15689778327941895, 0.15689781308174133, 0.15689769387245178, 0.15689769387245178, 0.15689773857593536, 0.15689772367477417, 0.15689776837825775, 0.15689772367477417, 0.15689758956432343, 0.1568976491689682, 0.1568976789712906, 0.1568974256515503, 0.1568978726863861, 0.15689760446548462, 0.1568973809480667, 0.15689751505851746, 0.15689757466316223, 0.1568976491689682, 0.1568974107503891, 0.1568976789712906, 0.15689772367477417, 0.15689760446548462, 0.1568974256515503, 0.156897634267807, 0.1568976193666458, 0.1568976789712906, 0.15689782798290253, 0.15689760446548462, 0.15689776837825775, 0.15689760446548462, 0.15689778327941895, 0.15689781308174133, 0.15689769387245178, 0.15689769387245178, 0.15689773857593536, 0.15689772367477417, 0.15689776837825775, 0.15689772367477417, 0.15689818561077118, 0.15689785778522491, 0.15689800679683685, 0.15689796209335327, 0.1568981260061264, 0.1568979024887085, 0.15689776837825775, 0.1568979173898697, 0.1568978875875473, 0.1568979024887085, 0.15689784288406372, 0.1568979024887085, 0.15689818561077118, 0.15689803659915924, 0.15689784288406372, 0.15689805150032043, 0.15689782798290253, 0.15689805150032043, 0.15689802169799805, 0.1568979024887085, 0.15689806640148163, 0.1568976789712906, 0.15689809620380402, 0.15689817070960999, 0.15689803659915924, 0.15689805150032043, 0.15689802169799805, 0.15689796209335327, 0.15689800679683685, 0.1568978875875473, 0.1568969488143921, 0.15689703822135925, 0.15689721703529358, 0.15689709782600403, 0.15689751505851746, 0.1568971574306488, 0.1568971425294876, 0.1568971574306488, 0.15689720213413239, 0.15689727663993835, 0.1568971574306488, 0.15689720213413239, 0.15689729154109955, 0.15689724683761597, 0.15689706802368164, 0.1568971425294876, 0.15689711272716522, 0.15689717233181, 0.15689732134342194, 0.15689711272716522, 0.15689726173877716, 0.15689708292484283, 0.15689727663993835, 0.15689730644226074, 0.15689720213413239, 0.1568971872329712, 0.15689723193645477, 0.15689720213413239, 0.15689727663993835, 0.15689723193645477, 0.15689755976200104, 0.1568974107503891, 0.15689776837825775, 0.1568976640701294, 0.1568979173898697, 0.15689751505851746, 0.15689747035503387, 0.15689772367477417, 0.15689754486083984, 0.15689773857593536, 0.15689758956432343, 0.15689755976200104, 0.15689778327941895, 0.1568976640701294, 0.15689735114574432, 0.15689776837825775, 0.156897634267807, 0.15689770877361298, 0.15689784288406372, 0.1568976193666458, 0.15689776837825775, 0.1568976193666458, 0.15689779818058014, 0.15689781308174133, 0.15689772367477417, 0.15689770877361298, 0.15689775347709656, 0.15689773857593536, 0.15689778327941895, 0.15689773857593536, 0.15689729154109955, 0.1568971574306488, 0.15689751505851746, 0.15689733624458313, 0.15689747035503387, 0.1568971574306488, 0.1568969488143921, 0.1568971425294876, 0.15689733624458313, 0.15689723193645477, 0.15689724683761597, 0.1568974256515503, 0.15689747035503387, 0.15689748525619507, 0.15689730644226074, 0.1568973809480667, 0.15689736604690552, 0.1568974107503891, 0.15689755976200104, 0.15689735114574432, 0.15689750015735626, 0.15689733624458313, 0.15689751505851746, 0.15689754486083984, 0.1568974405527115, 0.1568973958492279, 0.15689747035503387, 0.15689745545387268, 0.15689750015735626, 0.15689745545387268, 0.15689721703529358, 0.15689712762832642, 0.15689751505851746, 0.1568976193666458, 0.15689772367477417, 0.15689736604690552, 0.15689736604690552, 0.1568971425294876, 0.15689733624458313, 0.15689751505851746, 0.15689733624458313, 0.15689720213413239, 0.15689748525619507, 0.15689750015735626, 0.15689732134342194, 0.1568973958492279, 0.15689736604690552, 0.1568974405527115, 0.15689757466316223, 0.15689735114574432, 0.15689751505851746, 0.15689733624458313, 0.15689752995967865, 0.15689755976200104, 0.15689745545387268, 0.1568974405527115, 0.15689750015735626, 0.15689748525619507, 0.15689752995967865, 0.15689747035503387, 0.15689721703529358, 0.15689712762832642, 0.15689751505851746, 0.1568976193666458, 0.15689772367477417, 0.15689736604690552, 0.15689736604690552, 0.1568971425294876, 0.15689733624458313, 0.15689751505851746, 0.15689733624458313, 0.15689720213413239, 0.15689748525619507, 0.15689750015735626, 0.15689732134342194, 0.1568973958492279, 0.15689736604690552, 0.1568974405527115, 0.15689757466316223, 0.15689735114574432, 0.15689751505851746, 0.15689733624458313, 0.15689752995967865, 0.15689755976200104, 0.15689745545387268, 0.1568974405527115, 0.15689750015735626, 0.15689748525619507, 0.15689752995967865, 0.15689747035503387]\n",
      "[0.2400000000000002, 0.08999999999999986, 0.08999999999999986, 0.6300000000000008, 0.16000000000000014, 0.09999999999999964, 0.16000000000000014, 0.4499999999999993, 0.09999999999999964, 0.4300000000000015, 0.20999999999999908, 0.2900000000000009, 0.09999999999999964, 0.5199999999999996, 0.4900000000000002, 0.14000000000000057, 0.08000000000000007, 0.4399999999999995, 0.29999999999999893, 0.16000000000000014, 0.08999999999999986, 0.08000000000000007, 0.6000000000000014, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.18, 0.16000000000000003, 0.3500000000000001, 0.1499999999999999, 0.71, 0.6800000000000002, 0.7799999999999998, 0.6199999999999997, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.14, 0.4, 0.11999999999999988, 0.3600000000000001, 0.30000000000000004, 0.8700000000000001, 0.19999999999999973, 0.3799999999999999, 0.13000000000000034, 0.7400000000000002, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.25, 0.54, 0.1499999999999999, 0.17000000000000015, 0.45999999999999974, 0.79, 0.22999999999999998, 0.29000000000000004, 0.20000000000000018, 0.3999999999999999, 0.07000000000000028, 0.5699999999999994, 0.28000000000000025, 0.1900000000000004, 0.75, 0.25, 0.1200000000000001, 0.5799999999999992, 0.4900000000000002, 0.5099999999999998, 0.6300000000000008, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.25, 0.54, 0.1499999999999999, 0.17000000000000015, 0.45999999999999974, 0.79, 0.22999999999999998, 0.29000000000000004, 0.20000000000000018, 0.3999999999999999, 0.07000000000000028, 0.5699999999999994, 0.28000000000000025, 0.1900000000000004, 0.75, 0.25, 0.1200000000000001, 0.5799999999999992, 0.4900000000000002, 0.5099999999999998, 0.6300000000000008, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.18000000000000005, 0.4999999999999999, 0.30000000000000004, 0.13000000000000012, 0.46999999999999975, 0.14000000000000012, 0.06000000000000005, 0.8399999999999999, 0.16000000000000014, 0.1200000000000001, 0.1499999999999999, 0.3599999999999999, 0.1599999999999997, 0.11000000000000032, 0.41999999999999993, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.18000000000000005, 0.4999999999999999, 0.30000000000000004, 0.13000000000000012, 0.46999999999999975, 0.14000000000000012, 0.06000000000000005, 0.8399999999999999, 0.16000000000000014, 0.1200000000000001, 0.1499999999999999, 0.3599999999999999, 0.1599999999999997, 0.11000000000000032, 0.41999999999999993, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5700000000000001, 0.24, 0.20999999999999996, 0.18999999999999995, 0.45999999999999996, 0.1200000000000001, 0.7599999999999998, 0.20999999999999996, 0.23000000000000043, 0.33999999999999986, 0.3600000000000003, 0.3199999999999994, 0.1200000000000001, 0.9400000000000004, 0.29000000000000004, 0.16999999999999993, 0.25, 0.16000000000000014, 0.15999999999999925, 0.40000000000000036, 0.1299999999999999, 0.10000000000000053, 0.39999999999999947, 0.4200000000000008, 0.10999999999999943, 0.17999999999999972, 0.7699999999999996, 0.4800000000000004, 0.75, 0.3000000000000007, 0.08000000000000007, 0.379999999999999, 0.14000000000000057, 0.5800000000000001, 0.41000000000000014, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.27, 0.3700000000000001, 0.25, 0.19999999999999996, 0.6199999999999999, 0.20999999999999996, 0.5500000000000003, 0.3999999999999999, 0.1499999999999999, 0.8300000000000001, 0.5599999999999996, 0.4300000000000006, 0.9799999999999995, 0.33999999999999986, 0.29000000000000004, 0.5200000000000005, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.21999999999999997, 0.44999999999999996, 0.33000000000000007, 0.1499999999999999, 0.15000000000000013, 0.20999999999999974, 0.33000000000000007, 0.4500000000000002, 1.4100000000000001, 0.1200000000000001, 0.7799999999999994, 0.13000000000000078, 0.8699999999999992, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12, 0.33999999999999997, 0.18999999999999995, 0.42999999999999994, 0.5000000000000002, 0.11999999999999966, 0.40000000000000036, 0.11999999999999966, 0.54, 0.1200000000000001, 0.29000000000000004, 0.5100000000000002, 0.5800000000000001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12, 0.33999999999999997, 0.18999999999999995, 0.42999999999999994, 0.5000000000000002, 0.11999999999999966, 0.40000000000000036, 0.11999999999999966, 0.54, 0.1200000000000001, 0.29000000000000004, 0.5100000000000002, 0.5800000000000001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "mse_30_model = torch.load('models/bert_l30_MSE.model')\n",
    "mse_30_model.eval()\n",
    "preds, labels = evaluate_model_performance(mse_30_model, loader_val, torch.nn.MSELoss())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/ipykernel_launcher.py:25: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "/usr/lib/python3.6/site-packages/ipykernel_launcher.py:26: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sum loss: 1.4990735054016113\n",
      "printing distribution of predictions and labels of batch 32\n",
      "[0.04563185200095177, 0.04563182592391968, 0.04563186317682266, 0.04563181847333908, 0.04563187435269356, 0.04563184827566147, 0.04563191533088684, 0.04563181847333908, 0.045631926506757736, 0.04563178867101669, 0.04563188925385475, 0.045631811022758484, 0.04563188552856445, 0.04563190042972565, 0.04563186317682266, 0.045631829649209976, 0.04563188552856445, 0.045631878077983856, 0.04563185200095177, 0.04563184082508087, 0.045631811022758484, 0.04563187435269356, 0.04563184082508087, 0.04563194140791893, 0.04563192278146744, 0.04563191533088684, 0.045631952583789825, 0.04563190042972565, 0.04563193768262863, 0.04563198611140251, 0.04563180357217789, 0.04563179612159729, 0.045631833374500275, 0.045631807297468185, 0.045631881803274155, 0.04563180357217789, 0.045631781220436096, 0.045631855726242065, 0.04563190042972565, 0.04563184082508087, 0.04563193395733833, 0.04563185200095177, 0.045631930232048035, 0.045631907880306244, 0.045631907880306244, 0.04563189297914505, 0.04563192278146744, 0.04563190042972565, 0.04563190042972565, 0.04563189297914505, 0.04563186317682266, 0.045631907880306244, 0.04563184827566147, 0.045631907880306244, 0.04563188552856445, 0.04563187062740326, 0.04563191905617714, 0.04563186317682266, 0.04563190042972565, 0.04563194513320923, 0.04563181847333908, 0.04563187062740326, 0.04563184827566147, 0.04563180357217789, 0.045631833374500275, 0.04563184082508087, 0.04563184827566147, 0.045631829649209976, 0.04563183709979057, 0.045631811022758484, 0.04563194885849953, 0.04563187062740326, 0.04563194513320923, 0.045631926506757736, 0.045631930232048035, 0.045631907880306244, 0.04563193768262863, 0.04563191533088684, 0.04563192278146744, 0.04563191533088684, 0.04563188552856445, 0.045631926506757736, 0.04563186317682266, 0.04563192278146744, 0.045631904155015945, 0.04563189297914505, 0.04563193395733833, 0.04563188552856445, 0.04563191905617714, 0.04563196748495102, 0.04563180357217789, 0.04563180357217789, 0.045631833374500275, 0.045631781220436096, 0.04563187062740326, 0.045631833374500275, 0.04563192278146744, 0.04563185200095177, 0.04563188552856445, 0.04563186690211296, 0.04563189297914505, 0.045631833374500275, 0.04563189297914505, 0.04563189297914505, 0.04563187435269356, 0.04563189670443535, 0.04563188552856445, 0.04563184827566147, 0.04563190042972565, 0.045631855726242065, 0.04563186317682266, 0.04563193768262863, 0.045631878077983856, 0.04563193395733833, 0.04563191533088684, 0.045631904155015945, 0.04563194140791893, 0.04563188552856445, 0.045631930232048035, 0.045631974935531616, 0.04563180357217789, 0.04563180357217789, 0.045631833374500275, 0.045631781220436096, 0.04563187062740326, 0.045631833374500275, 0.04563192278146744, 0.04563185200095177, 0.04563188552856445, 0.04563186690211296, 0.04563189297914505, 0.045631833374500275, 0.04563189297914505, 0.04563189297914505, 0.04563187435269356, 0.04563189670443535, 0.04563188552856445, 0.04563184827566147, 0.04563190042972565, 0.045631855726242065, 0.04563186317682266, 0.04563193768262863, 0.045631878077983856, 0.04563193395733833, 0.04563191533088684, 0.045631904155015945, 0.04563194140791893, 0.04563188552856445, 0.045631930232048035, 0.045631974935531616, 0.04563182592391968, 0.04563186317682266, 0.04563188552856445, 0.04563179612159729, 0.045631952583789825, 0.045631833374500275, 0.04563186317682266, 0.04563185200095177, 0.04563182592391968, 0.04563179612159729, 0.04563194140791893, 0.04563182219862938, 0.04563188925385475, 0.045631833374500275, 0.045631881803274155, 0.04563191533088684, 0.04563194885849953, 0.04563192278146744, 0.045631930232048035, 0.04563191533088684, 0.04563188552856445, 0.045631930232048035, 0.04563187062740326, 0.045631930232048035, 0.04563191160559654, 0.04563189297914505, 0.04563193768262863, 0.04563188552856445, 0.04563192278146744, 0.045631974935531616, 0.04563182592391968, 0.04563186317682266, 0.04563188552856445, 0.04563179612159729, 0.045631952583789825, 0.045631833374500275, 0.04563186317682266, 0.04563185200095177, 0.04563182592391968, 0.04563179612159729, 0.04563194140791893, 0.04563182219862938, 0.04563188925385475, 0.045631833374500275, 0.045631881803274155, 0.04563191533088684, 0.04563194885849953, 0.04563192278146744, 0.045631930232048035, 0.04563191533088684, 0.04563188552856445, 0.045631930232048035, 0.04563187062740326, 0.045631930232048035, 0.04563191160559654, 0.04563189297914505, 0.04563193768262863, 0.04563188552856445, 0.04563192278146744, 0.045631974935531616, 0.04563192278146744, 0.045631878077983856, 0.04563188552856445, 0.045631833374500275, 0.04563188925385475, 0.04563184827566147, 0.045631974935531616, 0.045631855726242065, 0.04563188925385475, 0.04563184827566147, 0.045631956309080124, 0.045631833374500275, 0.04563192278146744, 0.04563192278146744, 0.04563193768262863, 0.04563188552856445, 0.045631930232048035, 0.04563194885849953, 0.04563192278146744, 0.04563189297914505, 0.04563186690211296, 0.04563189297914505, 0.04563184827566147, 0.04563188925385475, 0.04563187062740326, 0.04563184082508087, 0.04563191533088684, 0.04563191160559654, 0.04563190042972565, 0.04563196748495102, 0.04563179984688759, 0.04563181474804878, 0.04563182219862938, 0.045631781220436096, 0.04563186690211296, 0.04563187062740326, 0.04563191160559654, 0.04563188552856445, 0.04563189670443535, 0.04563184082508087, 0.045631930232048035, 0.04563184827566147, 0.045631930232048035, 0.045631907880306244, 0.045631907880306244, 0.04563189297914505, 0.04563192278146744, 0.04563189670443535, 0.04563190042972565, 0.04563189297914505, 0.04563186317682266, 0.045631907880306244, 0.04563184827566147, 0.045631907880306244, 0.04563188552856445, 0.045631878077983856, 0.04563191533088684, 0.04563186317682266, 0.04563190042972565, 0.04563194513320923, 0.045631833374500275, 0.04563182219862938, 0.04563189297914505, 0.04563182592391968, 0.045631881803274155, 0.04563183709979057, 0.04563187062740326, 0.04563187062740326, 0.04563182592391968, 0.04563184827566147, 0.04563190042972565, 0.04563181474804878, 0.04563193395733833, 0.04563181847333908, 0.04563186317682266, 0.045631833374500275, 0.04563194885849953, 0.04563192278146744, 0.045631930232048035, 0.04563191905617714, 0.04563188925385475, 0.04563193395733833, 0.04563187062740326, 0.045631930232048035, 0.045631907880306244, 0.04563190042972565, 0.04563194140791893, 0.04563188925385475, 0.04563192278146744, 0.045631974935531616, 0.04563184455037117, 0.045631878077983856, 0.04563187062740326, 0.04563184827566147, 0.04563189670443535, 0.045631833374500275, 0.045631878077983856, 0.04563191160559654, 0.04563184827566147, 0.04563179612159729, 0.04563193768262863, 0.045631811022758484, 0.04563190042972565, 0.045631930232048035, 0.045631930232048035, 0.04563191905617714, 0.04563194513320923, 0.04563192278146744, 0.045631930232048035, 0.04563191905617714, 0.04563188925385475, 0.04563193395733833, 0.04563187062740326, 0.045631930232048035, 0.04563191160559654, 0.04563190042972565, 0.04563193768262863, 0.04563188925385475, 0.04563192278146744, 0.04563197121024132, 0.04563181847333908, 0.04563181474804878, 0.04563191533088684, 0.045631762593984604, 0.04563193395733833, 0.04563186317682266, 0.04563188552856445, 0.04563184082508087, 0.04563184082508087, 0.04563178867101669, 0.04563193395733833, 0.04563184827566147, 0.045631829649209976, 0.045631930232048035, 0.045631930232048035, 0.045631907880306244, 0.04563194513320923, 0.04563191905617714, 0.04563192278146744, 0.04563191160559654, 0.04563188552856445, 0.045631930232048035, 0.04563187062740326, 0.04563192278146744, 0.045631907880306244, 0.04563189297914505, 0.04563193395733833, 0.04563188552856445, 0.04563191905617714, 0.04563196748495102, 0.04563181847333908, 0.04563181474804878, 0.04563191533088684, 0.045631762593984604, 0.04563193395733833, 0.04563186317682266, 0.04563188552856445, 0.04563184082508087, 0.04563184082508087, 0.04563178867101669, 0.04563193395733833, 0.04563184827566147, 0.045631829649209976, 0.045631930232048035, 0.045631930232048035, 0.045631907880306244, 0.04563194513320923, 0.04563191905617714, 0.04563192278146744, 0.04563191160559654, 0.04563188552856445, 0.045631930232048035, 0.04563187062740326, 0.04563192278146744, 0.045631907880306244, 0.04563189297914505, 0.04563193395733833, 0.04563188552856445, 0.04563191905617714, 0.04563196748495102]\n",
      "[0.2400000000000002, 0.08999999999999986, 0.08999999999999986, 0.6300000000000008, 0.16000000000000014, 0.09999999999999964, 0.16000000000000014, 0.4499999999999993, 0.09999999999999964, 0.4300000000000015, 0.20999999999999908, 0.2900000000000009, 0.09999999999999964, 0.5199999999999996, 0.4900000000000002, 0.14000000000000057, 0.08000000000000007, 0.4399999999999995, 0.29999999999999893, 0.16000000000000014, 0.08999999999999986, 0.08000000000000007, 0.6000000000000014, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.18, 0.16000000000000003, 0.3500000000000001, 0.1499999999999999, 0.71, 0.6800000000000002, 0.7799999999999998, 0.6199999999999997, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.14, 0.4, 0.11999999999999988, 0.3600000000000001, 0.30000000000000004, 0.8700000000000001, 0.19999999999999973, 0.3799999999999999, 0.13000000000000034, 0.7400000000000002, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.25, 0.54, 0.1499999999999999, 0.17000000000000015, 0.45999999999999974, 0.79, 0.22999999999999998, 0.29000000000000004, 0.20000000000000018, 0.3999999999999999, 0.07000000000000028, 0.5699999999999994, 0.28000000000000025, 0.1900000000000004, 0.75, 0.25, 0.1200000000000001, 0.5799999999999992, 0.4900000000000002, 0.5099999999999998, 0.6300000000000008, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.25, 0.54, 0.1499999999999999, 0.17000000000000015, 0.45999999999999974, 0.79, 0.22999999999999998, 0.29000000000000004, 0.20000000000000018, 0.3999999999999999, 0.07000000000000028, 0.5699999999999994, 0.28000000000000025, 0.1900000000000004, 0.75, 0.25, 0.1200000000000001, 0.5799999999999992, 0.4900000000000002, 0.5099999999999998, 0.6300000000000008, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.18000000000000005, 0.4999999999999999, 0.30000000000000004, 0.13000000000000012, 0.46999999999999975, 0.14000000000000012, 0.06000000000000005, 0.8399999999999999, 0.16000000000000014, 0.1200000000000001, 0.1499999999999999, 0.3599999999999999, 0.1599999999999997, 0.11000000000000032, 0.41999999999999993, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.18000000000000005, 0.4999999999999999, 0.30000000000000004, 0.13000000000000012, 0.46999999999999975, 0.14000000000000012, 0.06000000000000005, 0.8399999999999999, 0.16000000000000014, 0.1200000000000001, 0.1499999999999999, 0.3599999999999999, 0.1599999999999997, 0.11000000000000032, 0.41999999999999993, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5700000000000001, 0.24, 0.20999999999999996, 0.18999999999999995, 0.45999999999999996, 0.1200000000000001, 0.7599999999999998, 0.20999999999999996, 0.23000000000000043, 0.33999999999999986, 0.3600000000000003, 0.3199999999999994, 0.1200000000000001, 0.9400000000000004, 0.29000000000000004, 0.16999999999999993, 0.25, 0.16000000000000014, 0.15999999999999925, 0.40000000000000036, 0.1299999999999999, 0.10000000000000053, 0.39999999999999947, 0.4200000000000008, 0.10999999999999943, 0.17999999999999972, 0.7699999999999996, 0.4800000000000004, 0.75, 0.3000000000000007, 0.08000000000000007, 0.379999999999999, 0.14000000000000057, 0.5800000000000001, 0.41000000000000014, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.27, 0.3700000000000001, 0.25, 0.19999999999999996, 0.6199999999999999, 0.20999999999999996, 0.5500000000000003, 0.3999999999999999, 0.1499999999999999, 0.8300000000000001, 0.5599999999999996, 0.4300000000000006, 0.9799999999999995, 0.33999999999999986, 0.29000000000000004, 0.5200000000000005, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.21999999999999997, 0.44999999999999996, 0.33000000000000007, 0.1499999999999999, 0.15000000000000013, 0.20999999999999974, 0.33000000000000007, 0.4500000000000002, 1.4100000000000001, 0.1200000000000001, 0.7799999999999994, 0.13000000000000078, 0.8699999999999992, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12, 0.33999999999999997, 0.18999999999999995, 0.42999999999999994, 0.5000000000000002, 0.11999999999999966, 0.40000000000000036, 0.11999999999999966, 0.54, 0.1200000000000001, 0.29000000000000004, 0.5100000000000002, 0.5800000000000001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12, 0.33999999999999997, 0.18999999999999995, 0.42999999999999994, 0.5000000000000002, 0.11999999999999966, 0.40000000000000036, 0.11999999999999966, 0.54, 0.1200000000000001, 0.29000000000000004, 0.5100000000000002, 0.5800000000000001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "l1_30_model = torch.load('models/bert_l30_L1.model')\n",
    "l1_30_model.eval()\n",
    "preds, labels = evaluate_model_performance(l1_30_model, loader_val, torch.nn.L1Loss())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 2.25 MiB (GPU 0; 15.90 GiB total capacity; 3.28 GiB already allocated; 5.56 MiB free; 7.04 MiB cached)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m----------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-25e9e62ea94d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msmoothl1_30_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'models/bert_l30_SmoothL1.model'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0msmoothl1_30_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_model_performance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msmoothl1_30_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloader_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSmoothL1Loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib64/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module)\u001b[0m\n\u001b[1;32m    365\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 367\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    368\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnew_fd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib64/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_load\u001b[0;34m(f, map_location, pickle_module)\u001b[0m\n\u001b[1;32m    536\u001b[0m     \u001b[0munpickler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnpickler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m     \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpersistent_load\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpersistent_load\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 538\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    539\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m     \u001b[0mdeserialized_storage_keys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib64/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mpersistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m    502\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mroot_key\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdeserialized_objects\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m                 deserialized_objects[root_key] = restore_location(\n\u001b[0;32m--> 504\u001b[0;31m                     data_type(size), location)\n\u001b[0m\u001b[1;32m    505\u001b[0m             \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeserialized_objects\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mroot_key\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mview_metadata\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib64/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mdefault_restore_location\u001b[0;34m(storage, location)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdefault_restore_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_package_registry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib64/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_cuda_deserialize\u001b[0;34m(obj, location)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate_cuda_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib64/python3.6/site-packages/torch/_utils.py\u001b[0m in \u001b[0;36m_cuda\u001b[0;34m(self, device, non_blocking, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0mnew_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnew_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 2.25 MiB (GPU 0; 15.90 GiB total capacity; 3.28 GiB already allocated; 5.56 MiB free; 7.04 MiB cached)"
     ]
    }
   ],
   "source": [
    "smoothl1_30_model = torch.load('models/bert_l30_SmoothL1.model')\n",
    "smoothl1_30_model.eval()\n",
    "preds, labels = evaluate_model_performance(smoothl1_30_model, loader_val, torch.nn.SmoothL1Loss())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f2e160e7c18>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 842.4x595.44 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.distplot(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [0.04563185200095177, 0.04563182592391968, 0.04563186317682266, 0.04563181847333908, 0.04563187435269356, 0.04563184827566147, 0.04563191533088684, 0.04563181847333908, 0.045631926506757736, 0.04563178867101669, 0.04563188925385475, 0.045631811022758484, 0.04563188552856445, 0.04563190042972565, 0.04563186317682266, 0.045631829649209976, 0.04563188552856445, 0.045631878077983856, 0.04563185200095177, 0.04563184082508087, 0.045631811022758484, 0.04563187435269356, 0.04563184082508087, 0.04563194140791893, 0.04563192278146744, 0.04563191533088684, 0.045631952583789825, 0.04563190042972565, 0.04563193768262863, 0.04563198611140251, 0.04563180357217789, 0.04563179612159729, 0.045631833374500275, 0.045631807297468185, 0.045631881803274155, 0.04563180357217789, 0.045631781220436096, 0.045631855726242065, 0.04563190042972565, 0.04563184082508087, 0.04563193395733833, 0.04563185200095177, 0.045631930232048035, 0.045631907880306244, 0.045631907880306244, 0.04563189297914505, 0.04563192278146744, 0.04563190042972565, 0.04563190042972565, 0.04563189297914505, 0.04563186317682266, 0.045631907880306244, 0.04563184827566147, 0.045631907880306244, 0.04563188552856445, 0.04563187062740326, 0.04563191905617714, 0.04563186317682266, 0.04563190042972565, 0.04563194513320923, 0.04563181847333908, 0.04563187062740326, 0.04563184827566147, 0.04563180357217789, 0.045631833374500275, 0.04563184082508087, 0.04563184827566147, 0.045631829649209976, 0.04563183709979057, 0.045631811022758484, 0.04563194885849953, 0.04563187062740326, 0.04563194513320923, 0.045631926506757736, 0.045631930232048035, 0.045631907880306244, 0.04563193768262863, 0.04563191533088684, 0.04563192278146744, 0.04563191533088684, 0.04563188552856445, 0.045631926506757736, 0.04563186317682266, 0.04563192278146744, 0.045631904155015945, 0.04563189297914505, 0.04563193395733833, 0.04563188552856445, 0.04563191905617714, 0.04563196748495102, 0.04563180357217789, 0.04563180357217789, 0.045631833374500275, 0.045631781220436096, 0.04563187062740326, 0.045631833374500275, 0.04563192278146744, 0.04563185200095177, 0.04563188552856445, 0.04563186690211296, 0.04563189297914505, 0.045631833374500275, 0.04563189297914505, 0.04563189297914505, 0.04563187435269356, 0.04563189670443535, 0.04563188552856445, 0.04563184827566147, 0.04563190042972565, 0.045631855726242065, 0.04563186317682266, 0.04563193768262863, 0.045631878077983856, 0.04563193395733833, 0.04563191533088684, 0.045631904155015945, 0.04563194140791893, 0.04563188552856445, 0.045631930232048035, 0.045631974935531616, 0.04563180357217789, 0.04563180357217789, 0.045631833374500275, 0.045631781220436096, 0.04563187062740326, 0.045631833374500275, 0.04563192278146744, 0.04563185200095177, 0.04563188552856445, 0.04563186690211296, 0.04563189297914505, 0.045631833374500275, 0.04563189297914505, 0.04563189297914505, 0.04563187435269356, 0.04563189670443535, 0.04563188552856445, 0.04563184827566147, 0.04563190042972565, 0.045631855726242065, 0.04563186317682266, 0.04563193768262863, 0.045631878077983856, 0.04563193395733833, 0.04563191533088684, 0.045631904155015945, 0.04563194140791893, 0.04563188552856445, 0.045631930232048035, 0.045631974935531616, 0.04563182592391968, 0.04563186317682266, 0.04563188552856445, 0.04563179612159729, 0.045631952583789825, 0.045631833374500275, 0.04563186317682266, 0.04563185200095177, 0.04563182592391968, 0.04563179612159729, 0.04563194140791893, 0.04563182219862938, 0.04563188925385475, 0.045631833374500275, 0.045631881803274155, 0.04563191533088684, 0.04563194885849953, 0.04563192278146744, 0.045631930232048035, 0.04563191533088684, 0.04563188552856445, 0.045631930232048035, 0.04563187062740326, 0.045631930232048035, 0.04563191160559654, 0.04563189297914505, 0.04563193768262863, 0.04563188552856445, 0.04563192278146744, 0.045631974935531616, 0.04563182592391968, 0.04563186317682266, 0.04563188552856445, 0.04563179612159729, 0.045631952583789825, 0.045631833374500275, 0.04563186317682266, 0.04563185200095177, 0.04563182592391968, 0.04563179612159729, 0.04563194140791893, 0.04563182219862938, 0.04563188925385475, 0.045631833374500275, 0.045631881803274155, 0.04563191533088684, 0.04563194885849953, 0.04563192278146744, 0.045631930232048035, 0.04563191533088684, 0.04563188552856445, 0.045631930232048035, 0.04563187062740326, 0.045631930232048035, 0.04563191160559654, 0.04563189297914505, 0.04563193768262863, 0.04563188552856445, 0.04563192278146744, 0.045631974935531616, 0.04563192278146744, 0.045631878077983856, 0.04563188552856445, 0.045631833374500275, 0.04563188925385475, 0.04563184827566147, 0.045631974935531616, 0.045631855726242065, 0.04563188925385475, 0.04563184827566147, 0.045631956309080124, 0.045631833374500275, 0.04563192278146744, 0.04563192278146744, 0.04563193768262863, 0.04563188552856445, 0.045631930232048035, 0.04563194885849953, 0.04563192278146744, 0.04563189297914505, 0.04563186690211296, 0.04563189297914505, 0.04563184827566147, 0.04563188925385475, 0.04563187062740326, 0.04563184082508087, 0.04563191533088684, 0.04563191160559654, 0.04563190042972565, 0.04563196748495102, 0.04563179984688759, 0.04563181474804878, 0.04563182219862938, 0.045631781220436096, 0.04563186690211296, 0.04563187062740326, 0.04563191160559654, 0.04563188552856445, 0.04563189670443535, 0.04563184082508087, 0.045631930232048035, 0.04563184827566147, 0.045631930232048035, 0.045631907880306244, 0.045631907880306244, 0.04563189297914505, 0.04563192278146744, 0.04563189670443535, 0.04563190042972565, 0.04563189297914505, 0.04563186317682266, 0.045631907880306244, 0.04563184827566147, 0.045631907880306244, 0.04563188552856445, 0.045631878077983856, 0.04563191533088684, 0.04563186317682266, 0.04563190042972565, 0.04563194513320923, 0.045631833374500275, 0.04563182219862938, 0.04563189297914505, 0.04563182592391968, 0.045631881803274155, 0.04563183709979057, 0.04563187062740326, 0.04563187062740326, 0.04563182592391968, 0.04563184827566147, 0.04563190042972565, 0.04563181474804878, 0.04563193395733833, 0.04563181847333908, 0.04563186317682266, 0.045631833374500275, 0.04563194885849953, 0.04563192278146744, 0.045631930232048035, 0.04563191905617714, 0.04563188925385475, 0.04563193395733833, 0.04563187062740326, 0.045631930232048035, 0.045631907880306244, 0.04563190042972565, 0.04563194140791893, 0.04563188925385475, 0.04563192278146744, 0.045631974935531616, 0.04563184455037117, 0.045631878077983856, 0.04563187062740326, 0.04563184827566147, 0.04563189670443535, 0.045631833374500275, 0.045631878077983856, 0.04563191160559654, 0.04563184827566147, 0.04563179612159729, 0.04563193768262863, 0.045631811022758484, 0.04563190042972565, 0.045631930232048035, 0.045631930232048035, 0.04563191905617714, 0.04563194513320923, 0.04563192278146744, 0.045631930232048035, 0.04563191905617714, 0.04563188925385475, 0.04563193395733833, 0.04563187062740326, 0.045631930232048035, 0.04563191160559654, 0.04563190042972565, 0.04563193768262863, 0.04563188925385475, 0.04563192278146744, 0.04563197121024132, 0.04563181847333908, 0.04563181474804878, 0.04563191533088684, 0.045631762593984604, 0.04563193395733833, 0.04563186317682266, 0.04563188552856445, 0.04563184082508087, 0.04563184082508087, 0.04563178867101669, 0.04563193395733833, 0.04563184827566147, 0.045631829649209976, 0.045631930232048035, 0.045631930232048035, 0.045631907880306244, 0.04563194513320923, 0.04563191905617714, 0.04563192278146744, 0.04563191160559654, 0.04563188552856445, 0.045631930232048035, 0.04563187062740326, 0.04563192278146744, 0.045631907880306244, 0.04563189297914505, 0.04563193395733833, 0.04563188552856445, 0.04563191905617714, 0.04563196748495102, 0.04563181847333908, 0.04563181474804878, 0.04563191533088684, 0.045631762593984604, 0.04563193395733833, 0.04563186317682266, 0.04563188552856445, 0.04563184082508087, 0.04563184082508087, 0.04563178867101669, 0.04563193395733833, 0.04563184827566147, 0.045631829649209976, 0.045631930232048035, 0.045631930232048035, 0.045631907880306244, 0.04563194513320923, 0.04563191905617714, 0.04563192278146744, 0.04563191160559654, 0.04563188552856445, 0.045631930232048035, 0.04563187062740326, 0.04563192278146744, 0.045631907880306244, 0.04563189297914505, 0.04563193395733833, 0.04563188552856445, 0.04563191905617714, 0.04563196748495102]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.04563185200095177,\n",
       " 0.04563182592391968,\n",
       " 0.04563186317682266,\n",
       " 0.04563181847333908,\n",
       " 0.04563187435269356,\n",
       " 0.04563184827566147,\n",
       " 0.04563191533088684,\n",
       " 0.04563181847333908,\n",
       " 0.045631926506757736,\n",
       " 0.04563178867101669,\n",
       " 0.04563188925385475,\n",
       " 0.045631811022758484,\n",
       " 0.04563188552856445,\n",
       " 0.04563190042972565,\n",
       " 0.04563186317682266,\n",
       " 0.045631829649209976,\n",
       " 0.04563188552856445,\n",
       " 0.045631878077983856,\n",
       " 0.04563185200095177,\n",
       " 0.04563184082508087,\n",
       " 0.045631811022758484,\n",
       " 0.04563187435269356,\n",
       " 0.04563184082508087,\n",
       " 0.04563194140791893,\n",
       " 0.04563192278146744,\n",
       " 0.04563191533088684,\n",
       " 0.045631952583789825,\n",
       " 0.04563190042972565,\n",
       " 0.04563193768262863,\n",
       " 0.04563198611140251,\n",
       " 0.04563180357217789,\n",
       " 0.04563179612159729,\n",
       " 0.045631833374500275,\n",
       " 0.045631807297468185,\n",
       " 0.045631881803274155,\n",
       " 0.04563180357217789,\n",
       " 0.045631781220436096,\n",
       " 0.045631855726242065,\n",
       " 0.04563190042972565,\n",
       " 0.04563184082508087,\n",
       " 0.04563193395733833,\n",
       " 0.04563185200095177,\n",
       " 0.045631930232048035,\n",
       " 0.045631907880306244,\n",
       " 0.045631907880306244,\n",
       " 0.04563189297914505,\n",
       " 0.04563192278146744,\n",
       " 0.04563190042972565,\n",
       " 0.04563190042972565,\n",
       " 0.04563189297914505,\n",
       " 0.04563186317682266,\n",
       " 0.045631907880306244,\n",
       " 0.04563184827566147,\n",
       " 0.045631907880306244,\n",
       " 0.04563188552856445,\n",
       " 0.04563187062740326,\n",
       " 0.04563191905617714,\n",
       " 0.04563186317682266,\n",
       " 0.04563190042972565,\n",
       " 0.04563194513320923,\n",
       " 0.04563181847333908,\n",
       " 0.04563187062740326,\n",
       " 0.04563184827566147,\n",
       " 0.04563180357217789,\n",
       " 0.045631833374500275,\n",
       " 0.04563184082508087,\n",
       " 0.04563184827566147,\n",
       " 0.045631829649209976,\n",
       " 0.04563183709979057,\n",
       " 0.045631811022758484,\n",
       " 0.04563194885849953,\n",
       " 0.04563187062740326,\n",
       " 0.04563194513320923,\n",
       " 0.045631926506757736,\n",
       " 0.045631930232048035,\n",
       " 0.045631907880306244,\n",
       " 0.04563193768262863,\n",
       " 0.04563191533088684,\n",
       " 0.04563192278146744,\n",
       " 0.04563191533088684,\n",
       " 0.04563188552856445,\n",
       " 0.045631926506757736,\n",
       " 0.04563186317682266,\n",
       " 0.04563192278146744,\n",
       " 0.045631904155015945,\n",
       " 0.04563189297914505,\n",
       " 0.04563193395733833,\n",
       " 0.04563188552856445,\n",
       " 0.04563191905617714,\n",
       " 0.04563196748495102,\n",
       " 0.04563180357217789,\n",
       " 0.04563180357217789,\n",
       " 0.045631833374500275,\n",
       " 0.045631781220436096,\n",
       " 0.04563187062740326,\n",
       " 0.045631833374500275,\n",
       " 0.04563192278146744,\n",
       " 0.04563185200095177,\n",
       " 0.04563188552856445,\n",
       " 0.04563186690211296,\n",
       " 0.04563189297914505,\n",
       " 0.045631833374500275,\n",
       " 0.04563189297914505,\n",
       " 0.04563189297914505,\n",
       " 0.04563187435269356,\n",
       " 0.04563189670443535,\n",
       " 0.04563188552856445,\n",
       " 0.04563184827566147,\n",
       " 0.04563190042972565,\n",
       " 0.045631855726242065,\n",
       " 0.04563186317682266,\n",
       " 0.04563193768262863,\n",
       " 0.045631878077983856,\n",
       " 0.04563193395733833,\n",
       " 0.04563191533088684,\n",
       " 0.045631904155015945,\n",
       " 0.04563194140791893,\n",
       " 0.04563188552856445,\n",
       " 0.045631930232048035,\n",
       " 0.045631974935531616,\n",
       " 0.04563180357217789,\n",
       " 0.04563180357217789,\n",
       " 0.045631833374500275,\n",
       " 0.045631781220436096,\n",
       " 0.04563187062740326,\n",
       " 0.045631833374500275,\n",
       " 0.04563192278146744,\n",
       " 0.04563185200095177,\n",
       " 0.04563188552856445,\n",
       " 0.04563186690211296,\n",
       " 0.04563189297914505,\n",
       " 0.045631833374500275,\n",
       " 0.04563189297914505,\n",
       " 0.04563189297914505,\n",
       " 0.04563187435269356,\n",
       " 0.04563189670443535,\n",
       " 0.04563188552856445,\n",
       " 0.04563184827566147,\n",
       " 0.04563190042972565,\n",
       " 0.045631855726242065,\n",
       " 0.04563186317682266,\n",
       " 0.04563193768262863,\n",
       " 0.045631878077983856,\n",
       " 0.04563193395733833,\n",
       " 0.04563191533088684,\n",
       " 0.045631904155015945,\n",
       " 0.04563194140791893,\n",
       " 0.04563188552856445,\n",
       " 0.045631930232048035,\n",
       " 0.045631974935531616,\n",
       " 0.04563182592391968,\n",
       " 0.04563186317682266,\n",
       " 0.04563188552856445,\n",
       " 0.04563179612159729,\n",
       " 0.045631952583789825,\n",
       " 0.045631833374500275,\n",
       " 0.04563186317682266,\n",
       " 0.04563185200095177,\n",
       " 0.04563182592391968,\n",
       " 0.04563179612159729,\n",
       " 0.04563194140791893,\n",
       " 0.04563182219862938,\n",
       " 0.04563188925385475,\n",
       " 0.045631833374500275,\n",
       " 0.045631881803274155,\n",
       " 0.04563191533088684,\n",
       " 0.04563194885849953,\n",
       " 0.04563192278146744,\n",
       " 0.045631930232048035,\n",
       " 0.04563191533088684,\n",
       " 0.04563188552856445,\n",
       " 0.045631930232048035,\n",
       " 0.04563187062740326,\n",
       " 0.045631930232048035,\n",
       " 0.04563191160559654,\n",
       " 0.04563189297914505,\n",
       " 0.04563193768262863,\n",
       " 0.04563188552856445,\n",
       " 0.04563192278146744,\n",
       " 0.045631974935531616,\n",
       " 0.04563182592391968,\n",
       " 0.04563186317682266,\n",
       " 0.04563188552856445,\n",
       " 0.04563179612159729,\n",
       " 0.045631952583789825,\n",
       " 0.045631833374500275,\n",
       " 0.04563186317682266,\n",
       " 0.04563185200095177,\n",
       " 0.04563182592391968,\n",
       " 0.04563179612159729,\n",
       " 0.04563194140791893,\n",
       " 0.04563182219862938,\n",
       " 0.04563188925385475,\n",
       " 0.045631833374500275,\n",
       " 0.045631881803274155,\n",
       " 0.04563191533088684,\n",
       " 0.04563194885849953,\n",
       " 0.04563192278146744,\n",
       " 0.045631930232048035,\n",
       " 0.04563191533088684,\n",
       " 0.04563188552856445,\n",
       " 0.045631930232048035,\n",
       " 0.04563187062740326,\n",
       " 0.045631930232048035,\n",
       " 0.04563191160559654,\n",
       " 0.04563189297914505,\n",
       " 0.04563193768262863,\n",
       " 0.04563188552856445,\n",
       " 0.04563192278146744,\n",
       " 0.045631974935531616,\n",
       " 0.04563192278146744,\n",
       " 0.045631878077983856,\n",
       " 0.04563188552856445,\n",
       " 0.045631833374500275,\n",
       " 0.04563188925385475,\n",
       " 0.04563184827566147,\n",
       " 0.045631974935531616,\n",
       " 0.045631855726242065,\n",
       " 0.04563188925385475,\n",
       " 0.04563184827566147,\n",
       " 0.045631956309080124,\n",
       " 0.045631833374500275,\n",
       " 0.04563192278146744,\n",
       " 0.04563192278146744,\n",
       " 0.04563193768262863,\n",
       " 0.04563188552856445,\n",
       " 0.045631930232048035,\n",
       " 0.04563194885849953,\n",
       " 0.04563192278146744,\n",
       " 0.04563189297914505,\n",
       " 0.04563186690211296,\n",
       " 0.04563189297914505,\n",
       " 0.04563184827566147,\n",
       " 0.04563188925385475,\n",
       " 0.04563187062740326,\n",
       " 0.04563184082508087,\n",
       " 0.04563191533088684,\n",
       " 0.04563191160559654,\n",
       " 0.04563190042972565,\n",
       " 0.04563196748495102,\n",
       " 0.04563179984688759,\n",
       " 0.04563181474804878,\n",
       " 0.04563182219862938,\n",
       " 0.045631781220436096,\n",
       " 0.04563186690211296,\n",
       " 0.04563187062740326,\n",
       " 0.04563191160559654,\n",
       " 0.04563188552856445,\n",
       " 0.04563189670443535,\n",
       " 0.04563184082508087,\n",
       " 0.045631930232048035,\n",
       " 0.04563184827566147,\n",
       " 0.045631930232048035,\n",
       " 0.045631907880306244,\n",
       " 0.045631907880306244,\n",
       " 0.04563189297914505,\n",
       " 0.04563192278146744,\n",
       " 0.04563189670443535,\n",
       " 0.04563190042972565,\n",
       " 0.04563189297914505,\n",
       " 0.04563186317682266,\n",
       " 0.045631907880306244,\n",
       " 0.04563184827566147,\n",
       " 0.045631907880306244,\n",
       " 0.04563188552856445,\n",
       " 0.045631878077983856,\n",
       " 0.04563191533088684,\n",
       " 0.04563186317682266,\n",
       " 0.04563190042972565,\n",
       " 0.04563194513320923,\n",
       " 0.045631833374500275,\n",
       " 0.04563182219862938,\n",
       " 0.04563189297914505,\n",
       " 0.04563182592391968,\n",
       " 0.045631881803274155,\n",
       " 0.04563183709979057,\n",
       " 0.04563187062740326,\n",
       " 0.04563187062740326,\n",
       " 0.04563182592391968,\n",
       " 0.04563184827566147,\n",
       " 0.04563190042972565,\n",
       " 0.04563181474804878,\n",
       " 0.04563193395733833,\n",
       " 0.04563181847333908,\n",
       " 0.04563186317682266,\n",
       " 0.045631833374500275,\n",
       " 0.04563194885849953,\n",
       " 0.04563192278146744,\n",
       " 0.045631930232048035,\n",
       " 0.04563191905617714,\n",
       " 0.04563188925385475,\n",
       " 0.04563193395733833,\n",
       " 0.04563187062740326,\n",
       " 0.045631930232048035,\n",
       " 0.045631907880306244,\n",
       " 0.04563190042972565,\n",
       " 0.04563194140791893,\n",
       " 0.04563188925385475,\n",
       " 0.04563192278146744,\n",
       " 0.045631974935531616,\n",
       " 0.04563184455037117,\n",
       " 0.045631878077983856,\n",
       " 0.04563187062740326,\n",
       " 0.04563184827566147,\n",
       " 0.04563189670443535,\n",
       " 0.045631833374500275,\n",
       " 0.045631878077983856,\n",
       " 0.04563191160559654,\n",
       " 0.04563184827566147,\n",
       " 0.04563179612159729,\n",
       " 0.04563193768262863,\n",
       " 0.045631811022758484,\n",
       " 0.04563190042972565,\n",
       " 0.045631930232048035,\n",
       " 0.045631930232048035,\n",
       " 0.04563191905617714,\n",
       " 0.04563194513320923,\n",
       " 0.04563192278146744,\n",
       " 0.045631930232048035,\n",
       " 0.04563191905617714,\n",
       " 0.04563188925385475,\n",
       " 0.04563193395733833,\n",
       " 0.04563187062740326,\n",
       " 0.045631930232048035,\n",
       " 0.04563191160559654,\n",
       " 0.04563190042972565,\n",
       " 0.04563193768262863,\n",
       " 0.04563188925385475,\n",
       " 0.04563192278146744,\n",
       " 0.04563197121024132,\n",
       " 0.04563181847333908,\n",
       " 0.04563181474804878,\n",
       " 0.04563191533088684,\n",
       " 0.045631762593984604,\n",
       " 0.04563193395733833,\n",
       " 0.04563186317682266,\n",
       " 0.04563188552856445,\n",
       " 0.04563184082508087,\n",
       " 0.04563184082508087,\n",
       " 0.04563178867101669,\n",
       " 0.04563193395733833,\n",
       " 0.04563184827566147,\n",
       " 0.045631829649209976,\n",
       " 0.045631930232048035,\n",
       " 0.045631930232048035,\n",
       " 0.045631907880306244,\n",
       " 0.04563194513320923,\n",
       " 0.04563191905617714,\n",
       " 0.04563192278146744,\n",
       " 0.04563191160559654,\n",
       " 0.04563188552856445,\n",
       " 0.045631930232048035,\n",
       " 0.04563187062740326,\n",
       " 0.04563192278146744,\n",
       " 0.045631907880306244,\n",
       " 0.04563189297914505,\n",
       " 0.04563193395733833,\n",
       " 0.04563188552856445,\n",
       " 0.04563191905617714,\n",
       " 0.04563196748495102,\n",
       " 0.04563181847333908,\n",
       " 0.04563181474804878,\n",
       " 0.04563191533088684,\n",
       " 0.045631762593984604,\n",
       " 0.04563193395733833,\n",
       " 0.04563186317682266,\n",
       " 0.04563188552856445,\n",
       " 0.04563184082508087,\n",
       " 0.04563184082508087,\n",
       " 0.04563178867101669,\n",
       " 0.04563193395733833,\n",
       " 0.04563184827566147,\n",
       " 0.045631829649209976,\n",
       " 0.045631930232048035,\n",
       " 0.045631930232048035,\n",
       " 0.045631907880306244,\n",
       " 0.04563194513320923,\n",
       " 0.04563191905617714,\n",
       " 0.04563192278146744,\n",
       " 0.04563191160559654,\n",
       " 0.04563188552856445,\n",
       " 0.045631930232048035,\n",
       " 0.04563187062740326,\n",
       " 0.04563192278146744,\n",
       " 0.045631907880306244,\n",
       " 0.04563189297914505,\n",
       " 0.04563193395733833,\n",
       " 0.04563188552856445,\n",
       " 0.04563191905617714,\n",
       " 0.04563196748495102]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
